---
layout: post
title: Comment on Sharding in Ethereum
description: An alternate approach to sharding in Ethereum
---

*** UNDER CONSTRUCTION ***


Having read <a href="https://github.com/ethereum/wiki/wiki/Sharding-FAQ">Vitalik Buterin's FAQ</a> on sharding in Ethereum, I propose a alternate approach that seems more simple, robust, and versatile. I start by slicing the network horizontally instead of vertically. Instead of creating groups of nodes that are responsible for a subset of the state (shards), I propose to create groups of nodes that are responsible for the full state each. I.e. instead of having 50 shards with 100 nodes each, there would be 100 groups consisting of 50 subnodes each, with each group holding the full state. This won’t scale equally well as perfect sharding, but is much simpler to implement and more robust.

*My background*: as a former CTO of a distributed cloud storage startup, I have experienced many of the problems of executing transactions in distributed systems first hand. Thus, I think I have developed a gut feeling for what works well over the years. However, I am only tangentially familiar with how Ethereum is implemented and also did not read everything that was already said on the topic. So it is well possible that this proposal contains overlaps with previous ideas and maybe even some fundamental mistakes. If there is anything my past has taught me is that one should never be too confident about technical ideas before having actually implemented and tested them.

*Subnodes*: One of the great advantages of Ethereum in comparison to other cryptocurrencies is that the state is encoded in a Merkle tree,and its current root present in every block. This allows to create compact proofs about the current state. In comparison, it is not possible to prove that a given output is unspent in Bitcoin without scanning the whole transaction history since the creation of that output. This valuable property of Ethereum allows the creation of subnodes that are more powerful than normal SPV nodes as they are able to craft valid blocks with valid transactions in them. Subnodes are nodes that are only concerned with a subset of the state, and only download and validate transactions that are relevant to that subset. Whoever runs a subnode is free to specify what subset of the state he is interested in. By default, this can be a random subset, but it might also make sense for a specific users to track the state that is relevant to all the contracts he is involved in. This stands in contrast to Vitalik’s proposal, where nodes are exogenously assigned to a subset of the state (shard). By running such a subnode, a user would be able to monitor all the contracts relevant to him and even prove if something went wrong, i.e. if an invalid state transition was included in the block chain, he could provide the relevant transaction together with the Merkle-verified data before and after the transaction to prove to anyone that the transaction was incorrectly processed. Such a subnode could craft valid blocks that contain valid transactions, at least as long as these transactions only concern the state the node is tracking. This could even be expanded to simple "cross-shard" transactions if the node fetches the relevant data for those transaction from other nodes at O(log n) overhead. However, it gets problematic for transactions that depend on (a problem Vitalik also discusses).

*Block creation, boring variant*: In order to reliably validate old blocks and forge new blocks of transactions, subnodes have an incentive to form groups with other subnodes. Ideally, each group is large enough to cover the full state. Ideally, they also have some “mining power” among themselves, such that they can create new blocks. In the simplest case, such a group of nodes is actually just one supercomputer operated by a powerful miner and corresponds to today’s full nodes. But it also could be group of servers operated by the same miner. Or it could be a group of subnodes operated by people who trust each other. Or it could be a group of subnodes that found together similar to today’s mining pools. For groups of random nodes from the Internet, there obviously needs some mechanism to enforce correct behavior. The most simple one would be to expel subnodes that did not process a transaction correctly, which fortunately can easily be detected once a block created by that group gets declined by the other groups (other groups could even provide a proof for the wrong-doing). How these groups are formed and organized exactly is not important at this stage (proper processing of cross-shard transactions is somewhat hairy, but feasible with a well-defined ordering of transactions and efficient collaboration within the group). What is important is that there exists an incentive to behave well as a group and that there is a large number of such groups so they stand in competition with each other. Generally, having diversity in the way groups are formed (manually or automatically) makes the system as a whole more robust (for example, it makes it harder to infiltrate every group with a node that handles the same subset of data, which is the equivalent of taking over a whole shard in Vitalik's model). With this boring variant, one could already scale up one or even two orders of magnitude.

*Block creation, bold variant*: what would happen if subnodes worked on their own or if a group does not cover the whole state? In that case, they could still craft blocks consisting of transactions that operate on state they know, but they would not be able to fully verify previous blocks. So maybe it is time to separate the creation of new blocks from the verification of old blocks. Today, miners get rewarded for doing both at the same time. Here, I propose to split the rewards for block creation and block verification, and thus to have creating and verifying subnodes. So the creating subnode would just proceed and attach freshly crafted block to the blockchain when it is his turn to do so according to the proof-of-work or proof-of-stake mechanism. However, he risks losing his reward if it turns out that he built on top of an invalid chain. And if there are thousands of verifying subnodes that verify randomly chosen transactions (or some randomly chosen subset of the state), the chances are high that one of them will find out eventuall (maybe they coordinate in pools so not every node verifies the same transactions). If a verifying subnode finds an invalid block, the proof for the invalidity is created and incorporated into a new block at the same height as the invalid block. Ideally, the creator of the invalid block is punished, which would be feasible with Casper-style proof of stake, and the subnode that found the error rewarded. Obviously, subnodes can engage in both if they want, creating and verifying blocks. Also, subnodes could still form groups in order to advance a larger subset of the state in each block, but these groups do not necessarily need to cover the full state. What is still missing in the bold variant is ensuring data availability, which I implicitly assumed to be solved. In the boring variant with the complete groups, one can postulate that the group as a whole has the power to download and verfiy whole blocks and thus implicitely solve data availability the same way it is solved today, namely to refuse to build on top of a block whose relevant data is not know. In the bold variant, we need a new mechanism to ensure data availability. But appart from that, I think it could actually hold water and represent a more simple and more versatile approach to scaling up to tens of thousands of transactions per second.

*Data availability*: This is basically the missing piece of the puzzle - and I am honestly not sure how it is ideally solved. In simple systems, nodes refuse to create blocks without knowing everything about the previous block (unless they do risky "SPV mining"). With blocks that are too large for individual nodes and even for organized groups of nodes to process, this simple mechanism does not work any more. The fundamental problem here is that it is impossible to prove that a node did not provide some data on request. 

*Conclusion*: The advantage of this approach is that it provides a simple and robust path to scaling, while avoiding the synchronization headaches associated with cross-shard transactions. Agreeing on a “cross-shard” transaction within a small group of nodes is much easier than across large distributed shards. Still, large transactions that touch a too wide subset of the global state might have a hard time finding a group of subnodes that possesses all the state to execute that transaction. Execution of such transactions might only be worthwhile if they come with a very high fee that compensates for having to fetch additional state from other groups - which is good as costly operations are supposed to be expensive. Note that validation 
